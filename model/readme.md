# Model 

### Download the model

I recommand using the 1.5B model. of course, you can choose another model depending on you device, but it must meet these requirements:

1. It must be a DeepSeek-R1 distill model.
2. It must be in MLX format.
3.	Your device must be capable of running it (with sufficient memory).

You can find the models on [huggingface](https://huggingface.co/mlx-community?search_models=deepseek-r1-distill) 


Examples:

- [mlx-community/deepseek-r1-distill-qwen-1.5b](https://huggingface.co/mlx-community/deepseek-r1-distill-qwen-1.5b)
- [mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit)
- [mlx-community/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-8B)


# 模型

### 下载模型

我建议使用 1.5B 模型。当然，你也可以根据你的设备选择其他模型，但必须满足以下要求：

1. 必须是 DeepSeek-R1 distill 模型。
2. 必须是 MLX 格式。
3. 你的设备必须能够运行（内存够大）。

你可以在 [Hugging Face](https://huggingface.co/mlx-community?search_models=deepseek-r1-distill) 上找到这些模型。

例如：

- [mlx-community/deepseek-r1-distill-qwen-1.5b](https://huggingface.co/mlx-community/deepseek-r1-distill-qwen-1.5b)
- [mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit)
- [mlx-community/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Llama-8B)